# coding: utf-8
"""
ç”»åƒä¸€æ‹¬å–å¾—ã‚·ã‚¹ãƒ†ãƒ  - Webã‚¢ãƒ—ãƒªç‰ˆ
Flask APIã‚µãƒ¼ãƒãƒ¼
"""
from flask import Flask, request, send_file, jsonify, send_from_directory
from flask_cors import CORS
import tempfile
import zipfile
import os
import shutil
import time
import threading
from pathlib import Path
from playwright.sync_api import sync_playwright

# æ—¢å­˜ã®é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’å‰æï¼‰
from ç”»åƒä¸€æ‹¬å–å¾— import scrape_single_url_js

app = Flask(__name__)
CORS(app, expose_headers=['X-Success-URLs', 'X-Failed-URLs'])

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç”¨
temp_files_to_cleanup = []

def cleanup_temp_files():
    """å¤ã„ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆ5åˆ†çµŒéå¾Œï¼‰"""
    while True:
        time.sleep(60)  # 1åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
        now = time.time()
        for temp_dir, created_at in list(temp_files_to_cleanup):
            if now - created_at > 300:  # 5åˆ†çµŒéã—ãŸã‚‰å‰Šé™¤
                try:
                    if os.path.exists(temp_dir):
                        shutil.rmtree(temp_dir)
                    temp_files_to_cleanup.remove((temp_dir, created_at))
                except:
                    pass

# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’èµ·å‹•
cleanup_thread = threading.Thread(target=cleanup_temp_files, daemon=True)
cleanup_thread.start()

@app.route('/api/scrape', methods=['POST'])
def scrape():
    """
    URLãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ç”»åƒå–å¾—å‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™
    """
    try:
        data = request.get_json()
        urls = data.get('urls', [])
        
        if not urls:
            return jsonify({'error': 'URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
        
        # URLã®æ¤œè¨¼
        validated_urls = []
        for url in urls:
            url = url.strip()
            if url and not url.startswith('#'):
                if url.startswith('http://') or url.startswith('https://'):
                    validated_urls.append(url)
                else:
                    # http://ã‚’è‡ªå‹•è¿½åŠ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    validated_urls.append('https://' + url)
        
        if not validated_urls:
            return jsonify({'error': 'æœ‰åŠ¹ãªURLãŒã‚ã‚Šã¾ã›ã‚“'}), 400
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        temp_dir = tempfile.mkdtemp()
        result_root = os.path.join(temp_dir, 'result_js')
        os.makedirs(result_root, exist_ok=True)
        
        # æˆåŠŸ/å¤±æ•—ã‚’è¨˜éŒ²
        success_urls = []
        failed_urls = []

        try:
            # Playwrightã§ç”»åƒå–å¾—å‡¦ç†ã‚’å®Ÿè¡Œ
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                for url in validated_urls:
                    try:
                        scrape_single_url_js(url, result_root, browser)
                        success_urls.append(url)
                    except Exception as e:
                        print(f"Error processing {url}: {e}")
                        failed_urls.append({'url': url, 'error': str(e)})
                        continue
                browser.close()

            # çµæœã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            summary_path = os.path.join(result_root, '_result_summary.txt')
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write("ç”»åƒä¸€æ‹¬å–å¾—ã‚·ã‚¹ãƒ†ãƒ  - å‡¦ç†çµæœ\n")
                f.write("=" * 60 + "\n\n")

                f.write(f"ç·URLæ•°: {len(validated_urls)}\n")
                f.write(f"æˆåŠŸ: {len(success_urls)}ä»¶\n")
                f.write(f"å¤±æ•—: {len(failed_urls)}ä»¶\n\n")

                if success_urls:
                    f.write("-" * 60 + "\n")
                    f.write("âœ… æˆåŠŸã—ãŸURL:\n")
                    f.write("-" * 60 + "\n")
                    for url in success_urls:
                        f.write(f"  â€¢ {url}\n")
                    f.write("\n")

                if failed_urls:
                    f.write("-" * 60 + "\n")
                    f.write("âŒ å¤±æ•—ã—ãŸURL:\n")
                    f.write("-" * 60 + "\n")
                    for item in failed_urls:
                        f.write(f"  â€¢ {item['url']}\n")
                        f.write(f"    ã‚¨ãƒ©ãƒ¼: {item['error']}\n\n")
                    f.write("-" * 60 + "\n")
                    f.write("ğŸ’¡ ãƒ’ãƒ³ãƒˆ:\n")
                    f.write("å¤±æ•—ã—ãŸURLã¯æ–°ã—ã„extractorãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n")
                    f.write("Claude Codeã§ '/analyze-failed-url' Skillã‚’ä½¿ç”¨ã—ã¦\n")
                    f.write("URLæ§‹é€ ã‚’åˆ†æã§ãã¾ã™ã€‚\n")

            # çµæœã‚’ZIPåŒ–
            zip_path = os.path.join(temp_dir, 'result.zip')
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(result_root):
                    for file in files:
                        file_path = os.path.join(root, file)
                        # ç›¸å¯¾ãƒ‘ã‚¹ã§ZIPã«è¿½åŠ 
                        arcname = os.path.relpath(file_path, result_root)
                        zipf.write(file_path, arcname)
            
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆ5åˆ†å¾Œã«å‰Šé™¤ï¼‰
            temp_files_to_cleanup.append((temp_dir, time.time()))

            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™ï¼ˆæˆåŠŸ/å¤±æ•—æƒ…å ±ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã«å«ã‚ã‚‹ï¼‰
            import json
            response = send_file(
                zip_path,
                mimetype='application/zip',
                as_attachment=True,
                download_name='result.zip'
            )
            # æˆåŠŸã—ãŸURLãƒªã‚¹ãƒˆã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ 
            response.headers['X-Success-URLs'] = json.dumps(success_urls)
            # å¤±æ•—ã—ãŸURLãƒªã‚¹ãƒˆã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ ï¼ˆURLã®ã¿ï¼‰
            response.headers['X-Failed-URLs'] = json.dumps([item['url'] for item in failed_urls])
            return response
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å³åº§ã«å‰Šé™¤
            try:
                shutil.rmtree(temp_dir)
            except:
                pass
            raise
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/')
def index():
    """HTMLãƒšãƒ¼ã‚¸ã‚’è¿”ã™"""
    import os
    file_path = os.path.abspath('index.html')
    file_size = os.path.getsize(file_path)
    print(f"[INFO] Serving index.html from: {file_path}")
    print(f"[INFO] File size: {file_size} bytes")
    return send_from_directory('.', 'index.html')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
